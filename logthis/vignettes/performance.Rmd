---
title: "Performance and Optimization"
author: "logthis Team"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Performance and Optimization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
library(logthis)
```

## Introduction

Performance matters in logging - especially for high-volume applications. This vignette covers optimization strategies, benchmarking, and best practices for achieving high-performance logging with logthis.

Topics covered:

- Benchmarking results and expectations
- Buffered vs line-based receivers
- Async logging patterns
- Hot path optimization
- Memory management
- High-throughput strategies
- Performance troubleshooting

## Performance Characteristics

### Typical Latency (Single Event)

Based on benchmarks (R 4.5.1, Ubuntu 24.04, x86_64):

```
Receiver Type          | Latency      | Use Case
-----------------------|--------------|------------------
Identity (baseline)    | 5-10 µs      | Testing
Console                | 50-100 µs    | Development
Text file (sync)       | 200-500 µs   | Simple logging
JSON file (sync)       | 300-600 µs   | Structured logs
CSV file (sync)        | 400-700 µs   | Analysis
Async (queue time)     | 50-200 µs    | High volume
```

### Typical Throughput (1000 events)

```
Receiver Type          | Events/sec   | Speedup vs Sync
-----------------------|--------------|------------------
Console                | 10k-20k      | N/A
Text file (sync)       | 2k-5k        | 1x (baseline)
Text file (async)      | 20k-50k      | 5-10x
JSON file (sync)       | 1.5k-3k      | 1x
JSON file (async)      | 15k-40k      | 10-15x
Parquet (buffered)     | 5k-10k       | 2-3x
```

**Key takeaway:** Async receivers provide 5-20x throughput improvement for I/O-bound operations.

## Buffered vs Line-Based Receivers

### Line-Based Receivers (Immediate Write)

Write one line per event:

```{r}
# Text format
to_text() %>% on_local("app.log")

# JSON Lines format
to_json() %>% on_local("app.jsonl")

# CSV format
to_csv() %>% on_local("app.csv")
```

**Characteristics:**

- ✅ Simple implementation
- ✅ Events visible immediately
- ✅ Low memory usage
- ❌ Higher I/O overhead (one write per event)
- ❌ Lower throughput

**Use cases:**

- Development (immediate feedback)
- Low-volume production
- Real-time monitoring
- Critical audit logs (immediate persistence)

### Buffered Receivers (Batched Write)

Accumulate events in memory, write in batches:

```{r}
# Parquet format (columnar)
to_parquet() %>% on_local(
  "events.parquet",
  flush_threshold = 1000  # Write every 1000 events
)

# Feather format (columnar)
to_feather() %>% on_local(
  "events.feather",
  flush_threshold = 500
)
```

**Characteristics:**

- ✅ Higher throughput (fewer I/O operations)
- ✅ Better CPU efficiency
- ✅ Optimal for columnar formats
- ❌ Higher memory usage
- ❌ Events delayed until flush
- ❌ Risk of data loss if process crashes before flush

**Use cases:**

- High-volume production
- Batch processing
- Analytical workflows
- Cloud logging (reduce API calls)

### Choosing Flush Thresholds

Balance between throughput and latency:

```{r}
# Interactive/development: flush frequently
to_parquet() %>% on_local("app.parquet", flush_threshold = 10)
# Memory: ~10 events in buffer
# Latency: Events visible within ~10 events

# Production: balance
to_parquet() %>% on_local("app.parquet", flush_threshold = 100)
# Memory: ~100 events in buffer
# Latency: Events visible within ~100 events

# Batch processing: maximize throughput
to_parquet() %>% on_local("app.parquet", flush_threshold = 5000)
# Memory: ~5000 events in buffer
# Latency: Events visible within ~5000 events
```

**Rule of thumb:**

- Development: 10-50
- Production: 100-500
- Batch: 1000-5000

## Async Logging

Async receivers queue events for background processing, dramatically improving throughput:

### Basic Async Pattern

```{r}
library(mirai)  # Required for async

# Initialize daemon (do this once at app start)
mirai::daemons(1)

# Create async receiver
log_async <- logger() %>%
  with_receivers(
    to_text() %>% on_local("app.log") %>% as_async(flush_threshold = 100)
  )

# Log events (non-blocking!)
log_async(NOTE("Event 1"))  # Queued immediately, returns ~50µs
log_async(NOTE("Event 2"))  # Queued immediately
# ... writes happen in background daemon
```

**Performance impact:**

- **Sync:** 200-500 µs per event (blocks on I/O)
- **Async:** 50-200 µs per event (queue time only, 5-10x faster)

### Multiple Async Receivers

Use daemon pools for parallel processing:

```{r}
# Initialize daemon pool (4 workers)
mirai::daemons(4)

log_this <- logger() %>%
  with_receivers(
    # Worker 1: local file
    to_text() %>% on_local("app.log") %>% as_async(flush_threshold = 100),

    # Worker 2: JSON file
    to_json() %>% on_local("app.jsonl") %>% as_async(flush_threshold = 100),

    # Worker 3: S3 upload
    to_json() %>% on_s3("bucket", "key") %>% as_async(flush_threshold = 500),

    # Worker 4: Parquet
    to_parquet() %>% on_local("app.parquet") %>% as_async(flush_threshold = 1000)
  )

# All receivers process in parallel!
log_this(NOTE("High throughput event"))
```

**Speedup:** 5-20x for file I/O, 10-50x for network I/O

### Graceful Shutdown

Always flush before exit:

```{r}
# At app shutdown
flush_all_loggers()  # Wait for all queued events to complete

# Or manually
mirai::daemons(0)  # Stop daemons, wait for completion
```

### When to Use Async

**Use async when:**

- High event volume (>1000 events/sec)
- Network receivers (S3, webhooks)
- I/O-bound receivers (slow disks)
- Non-blocking is critical (web servers, APIs)

**Don't use async when:**

- Critical audit logs (need immediate persistence)
- Low volume (<100 events/sec)
- Simple console logging
- Process may crash (data loss risk)

## Hot Path Optimization

### 1. Level Checking Before Event Construction

Avoid constructing events that will be filtered:

```{r}
# BAD: Always constructs event (expensive!)
for (i in 1:10000) {
  log_this(DEBUG(
    "Processing item",
    item = i,
    expensive_field = compute_expensive_value(i)  # Always runs!
  ))
}

# GOOD: Check level first
should_debug <- .should_log(DEBUG)

for (i in 1:10000) {
  if (should_debug) {
    log_this(DEBUG(
      "Processing item",
      item = i,
      expensive_field = compute_expensive_value(i)
    ))
  }
}

# Helper function
.should_log <- function(level) {
  config <- attr(log_this, "config")
  lower <- config$lower %||% attr(LOWEST, "level_number")
  attr(level, "level_number") >= lower
}
```

**Performance impact:** 10-100x faster in hot paths when events are filtered

### 2. Minimize Custom Fields

Only include necessary metadata:

```{r}
# Hot path: minimal fields
process_items <- function(items) {
  for (item in items) {
    log_this(DEBUG("Processing", id = item$id))
    # ... process
  }
}

# Less frequent: rich metadata
complete_batch <- function(batch_id, items) {
  log_this(NOTE(
    "Batch complete",
    batch_id = batch_id,
    count = length(items),
    duration_ms = elapsed,
    memory_mb = pryr::mem_used() / 1e6,
    timestamp = Sys.time()
  ))
}
```

### 3. Use Appropriate Log Levels

Higher levels = fewer logs = better performance:

```{r}
# Development: verbose (slow)
log_this <- logger() %>%
  with_receivers(to_console(lower = DEBUG))

# Production: conservative (fast)
log_this <- logger() %>%
  with_receivers(to_console(lower = WARNING))
```

**Performance impact:** 10-1000x reduction in log volume by using WARNING instead of DEBUG

### 4. Receiver-Level Filtering

Filter at the receiver, not after:

```{r}
# BAD: All events go to both receivers
logger() %>%
  with_receivers(
    to_text() %>% on_local("all.log"),
    to_text() %>% on_local("errors.log")  # No filtering!
  )

# GOOD: Errors filtered early
logger() %>%
  with_receivers(
    to_text() %>% on_local("all.log"),
    to_text(lower = ERROR) %>% on_local("errors.log")  # Only ERRORs processed
  )
```

### 5. Lazy Evaluation in Middleware

Only process events that pass filters:

```{r}
expensive_middleware <- middleware(function(event) {
  # Early return for low-level events
  if (event$level_number < attr(WARNING, "level_number")) {
    return(event)
  }

  # Expensive processing only for warnings and above
  event$expensive_field <- compute_expensive_value()
  event
})
```

## Memory Management

### Memory Usage Patterns

Typical memory per event:

```
Component              | Memory/Event | Notes
-----------------------|--------------|------------------
Event object           | ~1 KB        | Base event
Console logging        | ~1-2 KB      | Formatting overhead
Text file (sync)       | ~1-2 KB      | Minimal buffering
JSON file (sync)       | ~2-3 KB      | Serialization
Parquet (buffered)     | ~1-2 KB      | Columnar storage
Async queue            | ~1-2 KB      | Queue overhead
```

**Total memory for 1000 buffered events:** ~1-3 MB

### Controlling Memory Usage

#### 1. Lower Flush Thresholds

```{r}
# High memory: 5000 events buffered
to_parquet() %>% on_local("app.parquet", flush_threshold = 5000)
# Memory: ~5-10 MB

# Low memory: 100 events buffered
to_parquet() %>% on_local("app.parquet", flush_threshold = 100)
# Memory: ~100-200 KB
```

#### 2. Limit Async Queue Depth

```{r}
# Deep queue (high memory, high throughput)
as_async(flush_threshold = 1000)
# Queue depth: up to 1000 events = ~1-2 MB

# Shallow queue (low memory, lower throughput)
as_async(flush_threshold = 100)
# Queue depth: up to 100 events = ~100-200 KB
```

#### 3. Use Line-Based Receivers for Low-Memory

```{r}
# Buffered: higher memory
to_parquet() %>% on_local("app.parquet", flush_threshold = 1000)

# Line-based: lower memory
to_json() %>% on_local("app.jsonl")  # No buffering
```

### Monitoring Memory Usage

```{r}
# Track memory
start_mem <- pryr::mem_used()

# ... logging operations ...

end_mem <- pryr::mem_used()
memory_increase_mb <- (end_mem - start_mem) / 1e6

cat("Memory increase:", memory_increase_mb, "MB\n")
```

## High-Throughput Strategies

### Strategy 1: Async + Buffering + Filtering

Combine multiple optimizations:

```{r}
library(mirai)
mirai::daemons(4)

log_this <- logger() %>%
  with_middleware(
    # Sample DEBUG logs (90% reduction)
    sample_by_level(debug_rate = 0.1)
  ) %>%
  with_receivers(
    # Async + large buffer
    to_parquet() %>%
      on_local("events.parquet", flush_threshold = 5000) %>%
      as_async(),

    # Only errors to separate file
    to_json(lower = ERROR) %>%
      on_local("errors.jsonl") %>%
      as_async(flush_threshold = 100)
  )
```

**Expected throughput:** 50k-100k events/sec

### Strategy 2: Separate Loggers for Hot Paths

Use minimal logger in hot paths:

```{r}
# Hot path logger (minimal)
log_hot <- logger() %>%
  with_receivers(
    to_text(lower = WARNING) %>% on_local("hot.log") %>% as_async()
  )

# Cold path logger (full featured)
log_cold <- logger() %>%
  with_middleware(
    add_system_context,
    add_performance_metrics
  ) %>%
  with_receivers(
    to_console(lower = DEBUG),
    to_parquet() %>% on_local("full.parquet", flush_threshold = 1000)
  )

# Use appropriately
for (i in 1:1000000) {
  log_hot(DEBUG("Processing", i = i))  # Fast
}

log_cold(NOTE("Batch complete"))  # Full context
```

### Strategy 3: Sampling

Reduce volume while maintaining visibility:

```{r}
sample_by_level <- function(debug_rate = 0.01, note_rate = 0.1) {
  middleware(function(event) {
    # Always keep warnings and above
    if (event$level_number >= attr(WARNING, "level_number")) {
      return(event)
    }

    # Sample lower levels
    sample_rate <- if (event$level_class == "DEBUG") {
      debug_rate
    } else {
      note_rate
    }

    if (runif(1) <= sample_rate) {
      event$sampled <- TRUE
      event$sample_rate <- sample_rate
      event
    } else {
      NULL  # Drop event
    }
  })
}

log_this <- logger() %>%
  with_middleware(
    sample_by_level(debug_rate = 0.01, note_rate = 0.1)
  ) %>%
  with_receivers(to_console())

# Result: 99% of DEBUG events dropped, 90% of NOTE events dropped
```

**Volume reduction:** 10-100x

### Strategy 4: Batch Processing

Process events in batches:

```{r}
# Accumulate events
events <- list()

for (i in 1:10000) {
  events[[i]] <- NOTE("Item processed", id = i)
}

# Log in batch
start_time <- Sys.time()

for (event in events) {
  log_this(event)
}

elapsed_ms <- as.numeric(Sys.time() - start_time) * 1000
throughput <- length(events) / (elapsed_ms / 1000)

cat("Throughput:", round(throughput), "events/sec\n")
```

## Benchmarking

### Running Benchmarks

```{r}
# Full benchmark suite (2-5 minutes)
source("benchmarks/benchmark_receivers.R")

# Quick benchmark (30 seconds)
source("benchmarks/quick_benchmark.R")
```

### Custom Benchmarks

```{r}
library(bench)

# Benchmark different receivers
results <- bench::mark(
  console = {
    log_console <- logger() %>%
      with_receivers(to_console())
    log_console(NOTE("Test"))
  },

  text_file = {
    log_file <- logger() %>%
      with_receivers(to_text() %>% on_local(tempfile()))
    log_file(NOTE("Test"))
  },

  json_file = {
    log_json <- logger() %>%
      with_receivers(to_json() %>% on_local(tempfile()))
    log_json(NOTE("Test"))
  },

  check = FALSE,
  min_iterations = 100
)

print(results)
```

### Interpreting Results

```{r}
# View results
results
#> # A tibble: 3 × 13
#>   expression      min median  itr/s mem_alloc gc/sec n_itr  n_gc
#>   <bch:expr> <bch:tm> <bch:> <dbl> <bch:byt>  <dbl> <int> <dbl>
#> 1 console      50.3µs  60.2µs 15000   120KB     2.3   1000    23
#> 2 text_file     200µs   250µs  3500   200KB     1.1   1000    11
#> 3 json_file     350µs   400µs  2300   280KB     0.9   1000     9

# Median: typical performance
# itr/s: throughput (events per second)
# mem_alloc: memory per event
```

## Performance Troubleshooting

### Problem: Logging is Slow

**Symptoms:** High latency, blocking behavior

**Solutions:**

1. **Use async receivers:**
   ```{r}
   to_text() %>% on_local("app.log") %>% as_async()
   ```

2. **Increase flush thresholds:**
   ```{r}
   to_parquet() %>% on_local("app.parquet", flush_threshold = 5000)
   ```

3. **Filter aggressively:**
   ```{r}
   logger() %>% with_receivers(to_console(lower = WARNING))
   ```

4. **Check level before logging:**
   ```{r}
   if (.should_log(DEBUG)) {
     log_this(DEBUG("Expensive message"))
   }
   ```

### Problem: High Memory Usage

**Symptoms:** Memory growth, OOM errors

**Solutions:**

1. **Lower flush thresholds:**
   ```{r}
   to_parquet() %>% on_local("app.parquet", flush_threshold = 100)
   ```

2. **Use line-based receivers:**
   ```{r}
   to_json() %>% on_local("app.jsonl")  # No buffering
   ```

3. **Sample events:**
   ```{r}
   logger() %>% with_middleware(sample_by_level(debug_rate = 0.01))
   ```

### Problem: Events Not Appearing

**Symptoms:** Missing events, delayed visibility

**Solutions:**

1. **Flush manually:**
   ```{r}
   flush_all_loggers()
   ```

2. **Lower flush threshold:**
   ```{r}
   flush_threshold = 10  # More frequent flushes
   ```

3. **Use sync receivers (development):**
   ```{r}
   to_text() %>% on_local("app.log")  # No async, immediate write
   ```

### Problem: High CPU Usage

**Symptoms:** CPU pegged, slow processing

**Solutions:**

1. **Reduce log volume:**
   ```{r}
   logger() %>% with_receivers(to_console(lower = WARNING))
   ```

2. **Sample events:**
   ```{r}
   sample_by_level(debug_rate = 0.01, note_rate = 0.1)
   ```

3. **Simplify middleware:**
   ```{r}
   # Remove expensive middleware in hot paths
   logger() %>%
     with_middleware(add_system_context)  # Only once
   ```

## Real-World Example: High-Volume API

```{r}
library(logthis)
library(mirai)

# Initialize async daemons
mirai::daemons(4)

# Create high-performance logger
log_api <- logger() %>%
  with_middleware(
    # Sample DEBUG aggressively
    sample_by_level(debug_rate = 0.01, note_rate = 0.1)
  ) %>%
  with_receivers(
    # All events → Parquet (async, large buffer)
    to_parquet() %>%
      on_local("api_events.parquet", flush_threshold = 5000) %>%
      as_async(),

    # Errors only → JSON (async, small buffer)
    to_json(lower = ERROR) %>%
      on_local("api_errors.jsonl", flush_threshold = 100) %>%
      as_async(),

    # Critical errors → Console (sync, immediate)
    to_console(lower = CRITICAL)
  )

# API handler
handle_request <- function(req) {
  # Check if we should log (hot path optimization)
  should_debug <- .should_log(DEBUG)

  if (should_debug) {
    log_api(DEBUG("Request received", path = req$path))
  }

  start_time <- Sys.time()

  tryCatch({
    result <- process_request(req)

    duration_ms <- as.numeric(Sys.time() - start_time) * 1000

    log_api(NOTE(
      "Request completed",
      path = req$path,
      status = 200,
      duration_ms = round(duration_ms, 1)
    ))

    result
  }, error = function(e) {
    log_api(ERROR(
      "Request failed",
      path = req$path,
      error = conditionMessage(e)
    ))

    stop(e)
  })
}

# Cleanup on shutdown
on.exit({
  flush_all_loggers()
  mirai::daemons(0)
})
```

**Expected performance:**

- Throughput: 50k-100k events/sec
- Latency: 50-200 µs per event (queue time)
- Memory: ~5-10 MB for buffered events
- Volume reduction: 90-99% via sampling

## Best Practices Summary

1. **Use async for high volume** - 5-20x speedup
2. **Buffer when possible** - Fewer I/O operations
3. **Filter aggressively** - Process only what you need
4. **Check levels before logging** - Skip expensive construction
5. **Sample in hot paths** - 10-100x volume reduction
6. **Monitor performance** - Benchmark regularly
7. **Flush on shutdown** - Avoid data loss
8. **Tune flush thresholds** - Balance latency vs throughput
9. **Use appropriate log levels** - WARNING in production, DEBUG in development
10. **Profile your code** - Identify logging bottlenecks

## See Also

- `vignette("getting-started")` - Introduction to logthis
- `vignette("patterns")` - Common logging patterns
- `vignette("middleware")` - Middleware for sampling
- `benchmarks/README.md` - Detailed benchmark documentation

## Summary

High-performance logging with logthis:

- **Async receivers** provide 5-20x throughput improvement
- **Buffered receivers** reduce I/O overhead
- **Hot path optimization** via level checking and minimal fields
- **Sampling** reduces volume by 10-100x
- **Memory management** via flush threshold tuning
- **Benchmarking** ensures performance meets requirements

For most applications, async + buffering + filtering provides optimal performance.
